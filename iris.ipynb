{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a3a5409",
   "metadata": {},
   "source": [
    "# Iris Dataset Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2bf2b7",
   "metadata": {},
   "source": [
    "### Step 1 - Importing Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771a8cb9",
   "metadata": {},
   "source": [
    "The first step before we do anything, will be to import the packages we need for this evaluation and data extractions. Packages are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59d4c11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb3c66e",
   "metadata": {},
   "source": [
    "As we can see, we still need Pandas to interact with the iris document as well as numpy for all our data manipulation (Although Pandas can do most of what Numpy does, I needed it for some specific issues I encountered at the beginning which we'll dive into later on).\n",
    "\n",
    "Then, for all our plots, we'll need matplotlib and seaborn (While seaborn can do what matplotlib does, I started off with matplotlib and only at the end, did I move to seaborn as it was the easiest way to achieve a pairplot)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d145ce4",
   "metadata": {},
   "source": [
    "### Step 2 - Reading our Data Set and organising it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475912e5",
   "metadata": {},
   "source": [
    "The Iris data set comes with no column names and using Pandas, we have no way of actually knowing what columns are what. Through the names txt file, we know what the attributes are but we need to name them in order to work with them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76f5cf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc168d66",
   "metadata": {},
   "source": [
    "After our columns have been named, we go ahead and open the data set in our folder with Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b560093e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sepal_length  sepal_width  petal_length  petal_width         species\n",
      "0             5.1          3.5           1.4          0.2     Iris-setosa\n",
      "1             4.9          3.0           1.4          0.2     Iris-setosa\n",
      "2             4.7          3.2           1.3          0.2     Iris-setosa\n",
      "3             4.6          3.1           1.5          0.2     Iris-setosa\n",
      "4             5.0          3.6           1.4          0.2     Iris-setosa\n",
      "..            ...          ...           ...          ...             ...\n",
      "145           6.7          3.0           5.2          2.3  Iris-virginica\n",
      "146           6.3          2.5           5.0          1.9  Iris-virginica\n",
      "147           6.5          3.0           5.2          2.0  Iris-virginica\n",
      "148           6.2          3.4           5.4          2.3  Iris-virginica\n",
      "149           5.9          3.0           5.1          1.8  Iris-virginica\n",
      "\n",
      "[150 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('iris.data', names=column_names)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f379a1",
   "metadata": {},
   "source": [
    "Now, it's way more clearer. I had some issues here because all the documentation I was checking was about csv files, not data files. It was Chatgpt that explained to me that I can apply the same reading method to data files as I would to csv files.\n",
    "\n",
    "__Prompt__ - I have iris.data what file format is that?\n",
    "\n",
    "__Chatgpt response__: \n",
    "_The iris.data file is essentially a plain text file formatted as comma-separated values (CSV)._\n",
    "_Even though it doesn't have a \".csv\" extension, it follows the same structure: each row represents a record, and the values are separated by commas._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f055db",
   "metadata": {},
   "source": [
    "We are also going to need to go ahead and assign all those values in each column to their own variables. This will make it easier to work with each feature later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aecab1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sepal_length = df['sepal_length']\n",
    "sepal_width  = df['sepal_width']\n",
    "petal_length = df['petal_length']\n",
    "petal_width  = df['petal_width']\n",
    "species      = df['species']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee0707b",
   "metadata": {},
   "source": [
    "### Step 3 - Outputting Summaries of variables\n",
    "#### Step 3.1 - Fixing df.describe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "981110c7",
   "metadata": {},
   "source": [
    "This was the hardest part of the project for me. With Pandas, we have access to a very handy function called .describe() which makes our lives incredibly easy when it comes\n",
    "to getting statistics and values out of the data set. \n",
    "\n",
    "The main issue is the presentation. I don't want the median displayed as 50%. By default, Pandas displays it like this. Unfortunately according to the pandas documentation, df.describe(percentiles=[]) still includes the 50%. this is more of a stylistic choice but I think we need to solve it. \n",
    "\n",
    "I then went ahead and found out about select_dtypes and .agg to basically get just the numbers, seeing that we're more interested in that rather than the species at the moment. \n",
    "and getting the aggregates of what I want. So, we first get the numeric variables only and get the aggregates of what we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "903f2472",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_df = df.select_dtypes(include=['number'])\n",
    "numeric_summary = numeric_df.agg(['count', 'mean', 'std', 'min', 'median', 'max'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e88f9cf",
   "metadata": {},
   "source": [
    "#### Step 3.2 - Creating the Summary txt files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19212a0a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
